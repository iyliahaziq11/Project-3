{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb35d688",
   "metadata": {},
   "source": [
    "# Evaluation and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0dc98",
   "metadata": {},
   "source": [
    "In this project, we have taken 2 approaches to detect fake news from subreddit posts. There were of course plenty of challenges and limitations, but it is a good initator for future modeling. \n",
    "\n",
    "**First approach**\n",
    "\n",
    "The first approach used non-text numerical features which we have been used to in earlier projects. This gives good intepretability and intuition. For example, we saw how factors like score, number of comments, and even time of post creation, had a strong impact on deciding whether a post is fake or not. This will be considered our baseline model.\n",
    "\n",
    "**Second approach**\n",
    "\n",
    "The second approach used textual features and Natural Language Processing (NLP) to vectorize the text data. Prediction was then made based on the vectorized words and its corresponding scores. While the baseline model (i.e. first approach) did achieve a high accuracy score of 0.910 our second approach was not far off with 0.894 accuracy score. \n",
    "- Furthermore, the baseline model relies on information that may be volatile and that takes up time to build. For example, a particular post may receive more comments during specific periods of time, and the number of comment will keep increasing especially when the post is newly created.\n",
    "- As such, with said limitations, our second approach is still favourable, since there is less likelihood of the text data itself being altered. \n",
    "\n",
    "In conclusion, our chosen model is the Multinomial Naive Bayes with TF-IDF vectorizor. The model can be considered a success as it achieves a high accuracy of 0.894. Eventhough the score is just slightly lower than our baseline model, its versatility and the limitations of the baseline model means that our eventual model will always be preferred. With more data to learn from, and from other sources as well, there is potential for even higher scores with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21622967",
   "metadata": {},
   "source": [
    "# Recommendation and Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2720b7",
   "metadata": {},
   "source": [
    "With the limitations mentioned, one way of improving the project could be to combine both approaches into 1. With more time in hand, ColumnTransformers can be used within Pipeline to determine the scores when textual data is combined with non-textual data. \n",
    "\n",
    "Also, words used in social media is ever-changing. One word can emerge with an entirely different meaning from normal usage. Including sentiment analysis this project could be useful in making the model more robust.\n",
    "\n",
    "Furthermore, our model should expand the horizon and accept texts from a variety of sources; either from other subreddits, or even beyond reddit itself. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
